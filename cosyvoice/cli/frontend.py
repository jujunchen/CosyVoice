# Copyright (c) 2024 Alibaba Inc (authors: Xiang Lyu)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from functools import partial
from typing import Generator
import json
import onnxruntime
import torch
import numpy as np
import whisper
from typing import Callable
import torchaudio.compliance.kaldi as kaldi
import os
import re
import inflect
import copy
from cosyvoice.utils.file_utils import logging, load_wav
from cosyvoice.utils.frontend_utils import contains_chinese, replace_blank, replace_corner_mark, remove_bracket, spell_out_number, split_paragraph, is_only_punctuation


class CosyVoiceFrontEnd:
    """
    CosyVoice前端处理类
    
    负责文本预处理、语音特征提取、说话人嵌入提取等功能。
    是连接用户输入和模型推理之间的桥梁。
    """

    def __init__(self,
                 get_tokenizer: Callable,
                 feat_extractor: Callable,
                 campplus_model: str,
                 speech_tokenizer_model: str,
                 spk2info: str = '',
                 allowed_special: str = 'all'):
        """
        初始化前端处理器
        
        Args:
            get_tokenizer (Callable): 获取文本分词器的函数
            feat_extractor (Callable): 特征提取器函数
            campplus_model (str): 说话人识别模型路径（用于提取说话人嵌入）
            speech_tokenizer_model (str): 语音分词器模型路径
            spk2info (str): 说话人信息文件路径
            allowed_special (str): 允许的特殊标记，默认'all'
        """
        self.tokenizer = get_tokenizer()
        self.feat_extractor = feat_extractor
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        option = onnxruntime.SessionOptions()
        option.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
        option.intra_op_num_threads = 1
        self.campplus_session = onnxruntime.InferenceSession(campplus_model, sess_options=option, providers=["CPUExecutionProvider"])
        self.speech_tokenizer_session = onnxruntime.InferenceSession(speech_tokenizer_model, sess_options=option,
                                                                     providers=["CUDAExecutionProvider" if torch.cuda.is_available() else
                                                                                "CPUExecutionProvider"])
        if os.path.exists(spk2info):
            self.spk2info = torch.load(spk2info, map_location=self.device)
        else:
            self.spk2info = {}
        self.allowed_special = allowed_special
        self.inflect_parser = inflect.engine()
        # NOTE compatible when no text frontend tool is avaliable
        try:
            import ttsfrd
            self.frd = ttsfrd.TtsFrontendEngine()
            ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
            assert self.frd.initialize('{}/../../pretrained_models/CosyVoice-ttsfrd/resource'.format(ROOT_DIR)) is True, \
                'failed to initialize ttsfrd resource'
            self.frd.set_lang_type('pinyinvg')
            self.text_frontend = 'ttsfrd'
            logging.info('use ttsfrd frontend')
        except:
            try:
                from wetext import Normalizer as ZhNormalizer
                from wetext import Normalizer as EnNormalizer
                self.zh_tn_model = ZhNormalizer(remove_erhua=False)
                self.en_tn_model = EnNormalizer()
                self.text_frontend = 'wetext'
                logging.info('use wetext frontend')
            except:
                self.text_frontend = ''
                logging.info('no frontend is avaliable')


    def _extract_text_token(self, text):
        """
        提取文本的token表示
        
        Args:
            text (str or Generator): 输入文本或文本生成器
            
        Returns:
            tuple: (文本token张量, 文本token长度)
        """
        if isinstance(text, Generator):
            logging.info('get tts_text generator, will return _extract_text_token_generator!')
            # NOTE add a dummy text_token_len for compatibility
            return self._extract_text_token_generator(text), torch.tensor([0], dtype=torch.int32).to(self.device)
        else:
            text_token = self.tokenizer.encode(text, allowed_special=self.allowed_special)
            text_token = torch.tensor([text_token], dtype=torch.int32).to(self.device)
            text_token_len = torch.tensor([text_token.shape[1]], dtype=torch.int32).to(self.device)
            return text_token, text_token_len

    def _extract_text_token_generator(self, text_generator):
        """
        处理文本生成器，逐个提取token
        
        Args:
            text_generator: 文本生成器对象
            
        Yields:
            单个文本token
        """
        for text in text_generator:
            text_token, _ = self._extract_text_token(text)
            for i in range(text_token.shape[1]):
                yield text_token[:, i: i + 1]

    def _extract_speech_token(self, prompt_wav):
        speech = load_wav(prompt_wav, 16000)
        """
        提取语音的token表示
        
        Args:
            speech (Tensor): 输入语音波形
            
        Returns:
            tuple: (语音token张量, 语音token长度)
        """
        assert speech.shape[1] / 16000 <= 30, 'do not support extract speech token for audio longer than 30s'
        feat = whisper.log_mel_spectrogram(speech, n_mels=128)
        speech_token = self.speech_tokenizer_session.run(None,
                                                         {self.speech_tokenizer_session.get_inputs()[0].name:
                                                          feat.detach().cpu().numpy(),
                                                          self.speech_tokenizer_session.get_inputs()[1].name:
                                                          np.array([feat.shape[2]], dtype=np.int32)})[0].flatten().tolist()
        speech_token = torch.tensor([speech_token], dtype=torch.int32).to(self.device)
        speech_token_len = torch.tensor([speech_token.shape[1]], dtype=torch.int32).to(self.device)
        return speech_token, speech_token_len

    def _extract_spk_embedding(self, prompt_wav):
        speech = load_wav(prompt_wav, 16000)
        """
        提取说话人嵌入向量
        
        Args:
            speech (Tensor): 输入语音波形
            
        Returns:
            Tensor: 说话人嵌入向量
        """
        feat = kaldi.fbank(speech,
                           num_mel_bins=80,
                           dither=0,
                           sample_frequency=16000)
        feat = feat - feat.mean(dim=0, keepdim=True)
        embedding = self.campplus_session.run(None,
                                              {self.campplus_session.get_inputs()[0].name: feat.unsqueeze(dim=0).cpu().numpy()})[0].flatten().tolist()
        embedding = torch.tensor([embedding]).to(self.device)
        return embedding

    def _extract_speech_feat(self, prompt_wav):
        speech = load_wav(prompt_wav, 24000)
        """
        提取语音特征
        
        Args:
            speech (Tensor): 输入语音波形
            
        Returns:
            tuple: (语音特征张量, 语音特征长度)
        """
        speech_feat = self.feat_extractor(speech).squeeze(dim=0).transpose(0, 1).to(self.device)
        speech_feat = speech_feat.unsqueeze(dim=0)
        speech_feat_len = torch.tensor([speech_feat.shape[1]], dtype=torch.int32).to(self.device)
        return speech_feat, speech_feat_len

    def text_normalize(self, text, split=True, text_frontend=True):
        """
        文本标准化处理
        
        Args:
            text (str): 输入文本
            split (bool): 是否分割文本为句子，默认True
            text_frontend (bool): 是否使用文本前端处理，默认True
            
        Returns:
            list or str: 处理后的文本列表或字符串
        """
        if isinstance(text, Generator):
            logging.info('get tts_text generator, will skip text_normalize!')
            return [text]
        # NOTE skip text_frontend when ssml symbol in text
        if '<|' in text and '|>' in text:
            text_frontend = False
        if text_frontend is False or text == '':
            return [text] if split is True else text
        text = text.strip()
        if self.text_frontend == 'ttsfrd':
            texts = [i["text"] for i in json.loads(self.frd.do_voicegen_frd(text))["sentences"]]
            text = ''.join(texts)
        else:
            if contains_chinese(text):
                if self.text_frontend == 'wetext':
                    text = self.zh_tn_model.normalize(text)
                text = text.replace("\n", "")
                text = replace_blank(text)
                text = replace_corner_mark(text)
                text = text.replace(".", "。")
                text = text.replace(" - ", "，")
                text = remove_bracket(text)
                text = re.sub(r'[，,、]+$', '。', text)
                # texts = list(split_paragraph(text, partial(self.tokenizer.encode, allowed_special=self.allowed_special), "zh", token_max_n=20,
                #                              token_min_n=10, merge_len=5, comma_split=True))
                texts = list(split_paragraph(text, partial(self.tokenizer.encode, allowed_special=self.allowed_special), "zh", token_max_n=80,
                                             token_min_n=60, merge_len=20, comma_split=True))
            else:
                if self.text_frontend == 'wetext':
                    text = self.en_tn_model.normalize(text)
                text = spell_out_number(text, self.inflect_parser)
                texts = list(split_paragraph(text, partial(self.tokenizer.encode, allowed_special=self.allowed_special), "en", token_max_n=80,
                                             token_min_n=60, merge_len=20, comma_split=True))
        texts = [i for i in texts if not is_only_punctuation(i)]
        return texts if split is True else text

    def frontend_sft(self, tts_text, spk_id):
        """
        SFT模式前端处理
        
        Args:
            tts_text (str): 待合成文本
            spk_id (str): 说话人ID
            
        Returns:
            dict: 模型输入字典
        """
        tts_text_token, tts_text_token_len = self._extract_text_token(tts_text)
        # 原来的配置会失真
        # embedding = self.spk2info[spk_id]['llm_embedding']
        # model_input = {'text': tts_text_token, 'text_len': tts_text_token_len, 'llm_embedding': embedding, 'flow_embedding': embedding}
        model_input = self.spk2info[spk_id]
        model_input['text'] = tts_text_token
        model_input['text_len'] = tts_text_token_len
        return model_input

    def frontend_zero_shot(self, tts_text, prompt_text, prompt_wav, resample_rate, zero_shot_spk_id):
        """
        零样本语音合成的前端处理函数
        
        Args:
            tts_text (str): 要合成的目标文本
            prompt_text (str): 提示文本，用于零样本合成中的语义引导
            prompt_wav (Tensor): 提示语音，采样率为16kHz的音频数据
            resample_rate (int): 目标重采样率
            zero_shot_spk_id (str): 零样本说话人ID，如果提供则使用预存的说话人信息
            
        Returns:
            dict: 包含所有模型输入信息的字典
        """
        # 提取目标文本的token及其长度
        tts_text_token, tts_text_token_len = self._extract_text_token(tts_text)
        
        # 如果没有指定预定义的说话人ID，则从提示语音中提取相关信息
        if zero_shot_spk_id == '':
            # 提取提示文本的token及其长度
            prompt_text_token, prompt_text_token_len = self._extract_text_token(prompt_text)
                        
            # 提取重采样后语音的声学特征和特征长度
            speech_feat, speech_feat_len = self._extract_speech_feat(prompt_wav)
            
            # 提取提示语音的语音token和token长度（基于16kHz原始音频）
            speech_token, speech_token_len = self._extract_speech_token(prompt_wav)
            
            # 针对CosyVoice2模型的特殊处理，强制speech_feat与speech_token长度比例为2:1
            if resample_rate == 24000:
                # cosyvoice2, force speech_feat % speech_token = 2
                token_len = min(int(speech_feat.shape[1] / 2), speech_token.shape[1])
                speech_feat, speech_feat_len[:] = speech_feat[:, :2 * token_len], 2 * token_len
                speech_token, speech_token_len[:] = speech_token[:, :token_len], token_len
                
            # 提取说话人嵌入向量（基于16kHz原始音频）
            embedding = self._extract_spk_embedding(prompt_wav)
            
            # 构建模型输入字典，包含文本、语音特征和嵌入信息
            model_input = {'prompt_text': prompt_text_token, 'prompt_text_len': prompt_text_token_len,
                           'llm_prompt_speech_token': speech_token, 'llm_prompt_speech_token_len': speech_token_len,
                           'flow_prompt_speech_token': speech_token, 'flow_prompt_speech_token_len': speech_token_len,
                           'prompt_speech_feat': speech_feat, 'prompt_speech_feat_len': speech_feat_len,
                           'llm_embedding': embedding, 'flow_embedding': embedding}
        else:
            # 如果指定了预定义的说话人ID，则直接使用预存的说话人信息
            model_input = self.spk2info[zero_shot_spk_id]
            
        # 将目标文本及其长度添加到模型输入中
        model_input['text'] = tts_text_token
        model_input['text_len'] = tts_text_token_len
        return model_input

    def frontend_cross_lingual(self, tts_text, prompt_wav, resample_rate, zero_shot_spk_id):
        """
        跨语言模式前端处理
        
        Args:
            tts_text (str): 待合成文本
            prompt_speech_16k (Tensor): 提示语音
            resample_rate (int): 重采样率
            zero_shot_spk_id (str): 零样本说话人ID
            
        Returns:
            dict: 模型输入字典
        """
        model_input = self.frontend_zero_shot(tts_text, '', prompt_wav, resample_rate, zero_shot_spk_id)
        # in cross lingual mode, we remove prompt in llm
        del model_input['prompt_text']
        del model_input['prompt_text_len']
        del model_input['llm_prompt_speech_token']
        del model_input['llm_prompt_speech_token_len']
        return model_input

    def frontend_instruct(self, tts_text, spk_id, instruct_text):
        """
        Instruct模式前端处理
        
        Args:
            tts_text (str): 待合成文本
            spk_id (str): 说话人ID
            instruct_text (str): 指令文本
            
        Returns:
            dict: 模型输入字典
        """
        model_input = self.frontend_sft(tts_text, spk_id)
        # in instruct mode, we remove spk_embedding in llm due to information leakage
        del model_input['llm_embedding']
        instruct_text_token, instruct_text_token_len = self._extract_text_token(instruct_text)
        model_input['prompt_text'] = instruct_text_token
        model_input['prompt_text_len'] = instruct_text_token_len
        return model_input

    def frontend_instruct2(self, tts_text, instruct_text, prompt_wav, resample_rate, zero_shot_spk_id):
        """
        CosyVoice2的增强版Instruct模式前端处理
        
        Args:
            tts_text (str): 待合成文本
            instruct_text (str): 指令文本
            prompt_speech_16k (Tensor): 提示语音
            resample_rate (int): 重采样率
            zero_shot_spk_id (str): 零样本说话人ID
            
        Returns:
            dict: 模型输入字典
        """
        model_input = self.frontend_zero_shot(tts_text, instruct_text, prompt_wav, resample_rate, zero_shot_spk_id)
        instruct_text_token, instruct_text_token_len = self._extract_text_token(instruct_text)

        model_input_new = copy.copy(model_input)
        model_input_new['prompt_text'] = instruct_text_token
        model_input_new['prompt_text_len'] = instruct_text_token_len
        del model_input_new['llm_prompt_speech_token']
        del model_input_new['llm_prompt_speech_token_len']
        return model_input_new

    def frontend_vc(self, source_speech_16k, prompt_wav, resample_rate):
        """
        语音转换模式前端处理
        
        Args:
            source_speech_16k (Tensor): 源语音
            prompt_speech_16k (Tensor): 提示语音
            resample_rate (int): 重采样率
            
        Returns:
            dict: 模型输入字典
        """
        prompt_speech_token, prompt_speech_token_len = self._extract_speech_token(prompt_wav)
        prompt_speech_feat, prompt_speech_feat_len = self._extract_speech_feat(prompt_wav)
        embedding = self._extract_spk_embedding(prompt_wav)
        source_speech_token, source_speech_token_len = self._extract_speech_token(source_speech_16k)
        model_input = {'source_speech_token': source_speech_token, 'source_speech_token_len': source_speech_token_len,
                       'flow_prompt_speech_token': prompt_speech_token, 'flow_prompt_speech_token_len': prompt_speech_token_len,
                       'prompt_speech_feat': prompt_speech_feat, 'prompt_speech_feat_len': prompt_speech_feat_len,
                       'flow_embedding': embedding}
        return model_input